<!-- Generated by SpecStory -->

# 不使用梯度检查点的显存需求 (2025-05-31 19:57:17)

_**User**_

不使用梯度检查点需要多少的显存？

---

_**Assistant**_

让我为您解答这个问题。

在当前的训练配置中，我看到代码使用了梯度检查点（gradient checkpointing）来节省显存。如果不使用梯度检查点，所需的显存会显著增加。具体需要多少显存取决于以下几个关键因素：

1. 模型大小：代码中使用的 `model_name` 是从配置文件读取的，但没有具体显示是哪个模型。不同规模的模型所需显存差异很大。

2. 当前的训练参数：
   - 批次大小 (batch_size) = 2
   - 梯度累积步数 (gradient_accumulation_steps) = 4
   - 使用了混合精度训练 (fp16 = True)
   - 最大提示长度 = 400
   - 最大生成长度 = 200
   - 每个提示生成4个样本

一般来说，关闭梯度检查点会导致显存使用量增加2-3倍。例如：
- 如果是使用类似 GPT-2 medium (345M参数) 的模型，关闭梯度检查点可能需要 12-16GB 显存
- 如果是使用更大的模型如 GPT-2 large (774M参数)，可能需要 20-24GB 显存
- 对于更大的模型，显存需求会进一步增加

建议：
1. 如果您想确切知道所需显存，建议：
   - 先用小批次尝试运行
   - 使用 nvidia-smi 监控显存使用情况
   - 逐步增加批次大小直到显存溢出

2. 如果显存有限，可以：
   - 保持使用梯度检查点
   - 减小批次大小
   - 增加梯度累积步数
   - 减小最大序列长度

如果您需要知道具体模型的显存需求，可以告诉我您使用的具体模型名称，我可以提供更精确的估算。

---

_**User**_

不使用梯度检查点需要多少的显存？

---

_**Assistant**_

看到配置文件后，我可以更准确地回答这个问题。

您使用的是 Qwen2.5-0.5B-Instruct 模型，这是一个约5亿参数的模型。基于当前的训练配置：

1. 模型参数：
   - Qwen2.5-0.5B (约5亿参数)
   - 最大序列长度：512 tokens

2. 训练参数：
   - batch_size = 2（train.py中设置）
   - gradient_accumulation_steps = 4
   - fp16 = True（使用半精度训练）
   - num_generations = 4

不使用梯度检查点的情况下，预估显存使用：
- 模型权重：约2GB
- 前向传播和反向传播中的激活值和梯度：约4-5GB
- 优化器状态（Adam）：约4GB
- 其他开销（缓冲区等）：约1GB

总计估算：约10-12GB显存

注意事项：
1. 这是一个预估值，实际使用可能会有波动
2. 使用fp16混合精度训练有助于减少显存使用
3. 如果显存不足，建议：
   - 继续使用梯度检查点
   - 或减小batch_size
   - 或增加gradient_accumulation_steps

如果您想精确知道显存使用量，建议使用 `nvidia-smi` 命令实时监控。

---

_**User**_

WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
这是什么报错？

---

_**Assistant**_

这不是一个错误，而是一个警告提示。让我用中文解释：

这个警告是说在训练参数中，`run_name`（运行名称）和`output_dir`（输出目录）被设置成了相同的值。具体来说：

1. `output_dir`：在您的代码中设置为 "./results"
```python:train.py
grpo_config = GRPOConfig(
    output_dir="./results",
    # ... 其他配置
)
```

2. `run_name`：没有被显式设置，所以默认使用了与 `output_dir` 相同的值

解决方案：
1. 如果您想消除这个警告，可以在 GRPOConfig 中明确设置不同的 `run_name`：
```python:train.py
grpo_config = GRPOConfig(
    output_dir="./results",
    run_name="qwen-motion-training",  # 添加这一行
    # ... 其他配置
)
```

2. 或者直接忽略这个警告，因为这不会影响训练过程

这个警告的目的主要是提醒用户可能忘记设置独特的运行名称，这个名称在使用实验追踪工具（如 wandb）时会很有用，可以帮助区分不同的训练运行。

---

_**User**_

我现在想用swanlab来可视化grpo的训练。
请帮我修改一下目前的代码


---

_**Assistant**_

好的，我来帮您将代码从 wandb 改为使用 swanlab 进行可视化。我会进行相应的修改：

首先，让我们修改代码：

---

```diff
  import os
  import json
- import wandb
+ import swanlab
  from trl import GRPOTrainer, GRPOConfig
  from src.data.dataset import MotionDataset
```

```diff
          config = yaml.safe_load(f)
      
-     # 初始化wandb
-     wandb.init(
-         project="motion-prediction-rl",
+     # 初始化swanlab
+     swanlab.init(
+         experiment_name=config.get("experiment_name", "motion-prediction-training"),
          config=config,
-         name=config.get("experiment_name", "motion-prediction-training")
      )
      
```

```diff
  
      # 创建数据集
-     train_dataset = MotionDataset(train_data)  # 移除 tokenizer_name 参数
+     train_dataset = MotionDataset(train_data)
      
      # 设置奖励函数
```

```diff
          output_dir="./results",
          run_name="qwen-motion-training",
-         learning_rate=1e-6,            # 从小的学习率开始
+         learning_rate=1e-6,
          num_train_epochs=10,
          per_device_train_batch_size=2,
          gradient_accumulation_steps=4,
          fp16=True,
-         # gradient_checkpointing=True,    
-         # gradient_checkpointing_kwargs= {
-         #     "use_reentrant": True 
-         # },
          
          # GRPO特定参数
-         beta=0.04,                     # KL系数，控制与参考模型的差异
-         epsilon=0.2,                   # 裁剪范围
-         num_iterations=1,              # 每个批次的迭代次数
-         loss_type="bnpo",             # 使用bnpo损失函数
-         scale_rewards=True,           # 对奖励进行标准化
-         reward_weights=config["reward_weights"],  # 从配置文件中读取奖励权重
+         beta=0.04,
+         epsilon=0.2,
+         num_iterations=1,
+         loss_type="bnpo",
+         scale_rewards=True,
+         reward_weights=config["reward_weights"],
          
          # 生成参数
-         max_prompt_length=400,        # 最大提示长度
-         max_completion_length=200,    # 最大生成长度
-         num_generations=4,            # 每个提示生成8个样本
-         temperature=0.7,             # 稍微降低温度以增加确定性
-         top_p=0.9,                   # 使用nucleus sampling
+         max_prompt_length=400,
+         max_completion_length=200,
+         num_generations=4,
+         temperature=0.7,
+         top_p=0.9,
          
          # 日志和可视化
-         logging_steps=10,            # 每10步记录一次日志
-         save_steps=100,             # 每100步保存一次模型
-         log_completions=True,       # 记录生成样本
-         wandb_log_unique_prompts=True,
+         logging_steps=10,
+         save_steps=100,
+         log_completions=True,
+         report_to="none",  # 禁用默认的wandb报告
          
          # 其他优化设置
-         disable_dropout=True,       # 训练时禁用dropout
-         mask_truncated_completions=False,  # 处理截断的生成结果
-     )
+         disable_dropout=True,
+         mask_truncated_completions=False,
+     )
+ 
+     # 创建自定义回调函数来记录到swanlab
+     class SwanlabCallback:
+         def on_log(self, args, state, control, logs=None, **kwargs):
+             if logs:
+                 # 记录训练指标
+                 metrics = {k: v for k, v in logs.items() if isinstance(v, (int, float))}
+                 swanlab.log(metrics)
  
      # 设置训练器
```

```diff
          train_dataset=train_dataset,
          reward_funcs=reward_funcs,
-         callbacks=[VisualizationCallback(visualizer)]
+         callbacks=[VisualizationCallback(visualizer), SwanlabCallback()]
      )
  
```

```diff
      trainer.save_model("./final_model")
      
-     # 关闭wandb
-     wandb.finish()
+     # 关闭swanlab
+     swanlab.finish()
  
  if __name__ == "__main__":
```

---

现在我们还需要修改配置文件，移除 wandb 相关的配置并添加 swanlab 的配置：

---

```diff
    save_steps: 100
  
+ # 实验配置
+ experiment_name: "qwen-motion-training"
+ swanlab:
+   log_dir: "./logs"
+   metrics_update_interval: 10  # 每10步更新一次指标
+   save_model_interval: 100     # 每100步保存一次模型
+ 
  # 测试配置
  testing:
```

```diff
  # 可视化配置
  visualization:
-   wandb:
-     project: "motion-prediction-rl"
-     entity: ""  # 替换为您的wandb用户名
    plotting:
      figure_size: [12, 6]
```

---

主要修改内容包括：

1. 将 wandb 替换为 swanlab
2. 添加了 SwanlabCallback 类来处理训练过程中的指标记录
3. 移除了 wandb 特定的配置参数
4. 添加了 swanlab 的配置选项
5. 禁用了默认的 wandb 报告系统

使用方法：
1. 首先需要安装 swanlab：
```bash
pip install swanlab
```

2. 运行训练脚本：
```bash
python train.py
```

3. 启动 swanlab UI 查看训练过程：
```bash
swanlab up
```

swanlab 会在 `./logs` 目录下保存训练日志和指标，您可以通过 Web 界面实时查看训练进度、损失曲线、奖励分布等信息。

这些修改保留了原有的训练逻辑，只是将可视化工具从 wandb 换成了 swanlab。swanlab 的优势在于：
1. 完全离线运行，不需要网络连接
2. 界面简洁直观
3. 资源占用更少
4. 配置更简单

需要注意的是，如果您在训练过程中遇到任何问题，可以查看 swanlab 的日志文件来进行调试。

---

_**User**_

我的项目中命名已经没有使用wandb了，为什么在命令行中，它还会跳出来？


---

_**Assistant**_

我看到问题了。虽然我们已经改用了 swanlab，但在代码中还残留着一些 wandb 相关的配置。让我帮您清理这些残留：

---

```diff
  
      # 创建数据集
-     train_dataset = MotionDataset(train_data)  # 移除 tokenizer_name 参数
+     train_dataset = MotionDataset(train_data)
      
      # 设置奖励函数
```

```diff
          output_dir="./results",
          run_name="qwen-motion-training",
-         learning_rate=1e-6,            # 从小的学习率开始
+         learning_rate=1e-6,
          num_train_epochs=10,
          per_device_train_batch_size=2,
          gradient_accumulation_steps=4,
          fp16=True,
-         # gradient_checkpointing=True,    
-         # gradient_checkpointing_kwargs= {
-         #     "use_reentrant": True 
-         # },
          
          # GRPO特定参数
-         beta=0.04,                     # KL系数，控制与参考模型的差异
-         epsilon=0.2,                   # 裁剪范围
-         num_iterations=1,              # 每个批次的迭代次数
-         loss_type="bnpo",             # 使用bnpo损失函数
-         scale_rewards=True,           # 对奖励进行标准化
-         reward_weights=config["reward_weights"],  # 从配置文件中读取奖励权重
+         beta=0.04,
+         epsilon=0.2,
+         num_iterations=1,
+         loss_type="bnpo",
+         scale_rewards=True,
+         reward_weights=config["reward_weights"],
          
          # 生成参数
-         max_prompt_length=400,        # 最大提示长度
-         max_completion_length=200,    # 最大生成长度
-         num_generations=4,            # 每个提示生成8个样本
-         temperature=0.7,             # 稍微降低温度以增加确定性
-         top_p=0.9,                   # 使用nucleus sampling
+         max_prompt_length=400,
+         max_completion_length=200,
+         num_generations=4,
+         temperature=0.7,
+         top_p=0.9,
          
          # 日志和可视化
-         logging_steps=10,            # 每10步记录一次日志
-         save_steps=100,             # 每100步保存一次模型
-         log_completions=True,       # 记录生成样本
-         wandb_log_unique_prompts=True,
+         logging_steps=10,
+         save_steps=100,
+         log_completions=True,
+         report_to="none",  # 禁用默认的wandb报告
          
          # 其他优化设置
-         disable_dropout=True,       # 训练时禁用dropout
-         mask_truncated_completions=False,  # 处理截断的生成结果
+         disable_dropout=True,
+         mask_truncated_completions=False,
      )
  
      # 实例化SwanLabCallback
      swanlab_callback = SwanLabCallback(
          project=config["project"], 
-         experiment_name=config["experiment_name"]
+         experiment_name=config["experiment_name"],
+         log_dir=config["swanlab"]["log_dir"]
      )
  
```

---

主要修改了以下内容：
1. 移除了所有 wandb 相关的配置项（如 `wandb_log_unique_prompts`）
2. 添加了 `report_to="none"` 来完全禁用默认的 wandb 报告系统
3. 为 SwanLabCallback 添加了 `log_dir` 参数
4. 清理了不必要的注释和废弃的配置

但是，我注意到可能还有一个问题。让我们检查一下您的项目依赖，因为警告信息表明 wandb 包仍然被安装了。建议您：

1. 首先卸载 wandb：
```bash
pip uninstall wandb
```

2. 确保您的项目依赖中没有 wandb。如果您有 requirements.txt 文件，请检查并移除其中的 wandb。

3. 如果您使用的是虚拟环境，可以查看所有已安装的包：
```bash
pip list | grep wandb
```

4. 如果您使用的是 trl 库，它可能会自动安装 wandb 作为依赖。在这种情况下，您可以在安装完 trl 后手动卸载 wandb：
```bash
pip install trl
pip uninstall wandb
```

这样应该可以完全移除 wandb 相关的提示。如果您想查看是否还有其他文件引用了 wandb，我可以帮您搜索整个项目。

---

_**User**_

这个是什么报错？

---

_**Assistant**_

这是一个数据加载错误。错误信息显示 `local variable 'current_batch' referenced before assignment`，意味着在尝试访问 `current_batch` 变量时，这个变量还没有被赋值。

这个错误通常出现的原因是：
1. 数据集为空
2. 数据加载器配置不正确
3. 批次大小设置不合适

让我们检查一下数据集的实现：

---

<details>
            <summary>Searched codebase "class MotionDataset implementation" • **4** results</summary>
        
| File | Lines |
|------|-------|
| `src/data/dataset.py` | L1-20 |
| `src/data/generator.py` | L56-87 |
| `src/data/generator.py` | L1-56 |
| `src/data/__init__.py` | L1-1 |

</details>

---

让我们也检查一下数据文件：

---

Read file: data/train/motion_data.json

---

现在我明白问题所在了。这个错误的主要原因是：

1. 数据集格式问题：
   - 您的 `MotionDataset` 类期望返回一个包含 "prompt"、"target" 和 "reasoning" 的字典
   - 但是您的 JSON 数据文件中的数据格式是 "input_ids"、"query" 和 "response"

让我们修改 `MotionDataset` 类来解决这个问题：

---

```diff
          item = self.data[idx]
          
-         # GRPO需要的格式：必须包含 prompt，其他字段可选
+         # 返回GRPO需要的格式
          return {
+             "input_ids": item["input_ids"],    # 原始输入
              "prompt": item["query"],           # 输入提示
-             "target": item["response"],        # 目标输出（用于计算奖励）
-             "reasoning": item.get("reasoning", "")  # 推理过程（用于计算奖励）
+             "target": item["response"],        # 目标输出
+             "completion": "",                  # 初始完成为空
          }
```

---

主要修改包括：
1. 添加了 `input_ids` 字段
2. 移除了可选的 `reasoning` 字段
3. 添加了 `completion` 字段（GRPO需要）
4. 确保字段名称与数据文件匹配

这些修改应该能解决数据加载错误。现在您可以重新运行训练脚本：

```bash
python train.py
```

如果您还遇到其他问题，请告诉我，我会继续帮您解决。

---

_**User**_

这个训练保存checkpoint需要的空间好大

---

_**Assistant**_

是的，模型checkpoint确实会占用较大空间。这是因为：

1. 您使用的是 Qwen2.5-0.5B 模型（约5亿参数）
2. 默认每100步（`save_steps=100`）保存一次模型
3. 每个checkpoint都会保存完整的模型状态

让我们优化一下保存策略：

---

```diff
          # 日志和可视化
          logging_steps=10,
-         save_steps=100,
+         save_steps=500,              # 增加保存间隔
+         save_total_limit=2,          # 最多保存2个checkpoint
+         save_safetensors=True,       # 使用safetensors格式保存（更小）
          log_completions=True,
          report_to="none",  # 禁用默认的wandb报告
```

---

主要修改了以下内容：

1. `save_steps=500`：将保存间隔从100步增加到500步，减少保存频率
2. `save_total_limit=2`：只保留最新的2个checkpoint，旧的会自动删除
3. `save_safetensors=True`：使用 safetensors 格式保存模型，这种格式比传统的 PyTorch 格式更节省空间

其他建议：

1. 如果您不需要中间checkpoint，可以只保存最后的模型：
```python
save_strategy="no"  # 训练过程中不保存
```

2. 如果您想按时间间隔保存，可以改用：
```python
save_strategy="epoch"  # 每个epoch保存一次
```

3. 如果硬盘空间实在紧张，可以考虑：
   - 使用模型量化（比如int8）
   - 只保存模型权重，不保存优化器状态
   - 使用模型压缩技术

您想采用哪种策略？我可以帮您进一步调整。

---

_**User**_

我目前的程序是一共训练多少step？这个如何计算？
对于这个项目，多少条训练集数据比较合适？

---

_**Assistant**_

让我帮您分析一下训练步数的计算和数据集大小的建议：

1. 训练步数计算：
目前您的配置是：
- `num_train_epochs = 10`（训练轮数）
- `per_device_train_batch_size = 1`（每个设备的批次大小）
- `gradient_accumulation_steps = 4`（梯度累积步数）
- 训练数据集大小：根据配置文件 `config.yaml` 中看到 `n_samples = 1`

总训练步数的计算公式：
```
总步数 = (数据集大小 / (batch_size * gradient_accumulation_steps)) * num_epochs
```
当前设置下：
```
总步数 = (1 / (1 * 4)) * 10 = 2.5 步
```

这个训练步数明显太少了。根据作业要求和GRPO的特点，我建议：

2. 数据集大小建议：
- 训练集：建议 1000-2000 条数据
  - 原因1：GRPO每个样本会生成多个回答（您设置的 `num_generations=4`）
  - 原因2：需要足够的数据来学习模式
  - 原因3：作业提示说"强化学习训练非常不稳定"，需要足够数据来稳定训练

让我帮您修改配置：

---

```diff
  data:
    train:
-     n_samples: 1
+     n_samples: 1000  # 增加到1000条训练数据
      v0_range: [0.5, 2.0]
      a_range: [0.1, 0.5]
    test_in:
-     n_samples: 1
+     n_samples: 100   # 增加到100条in-distribution测试数据
      v0_range: [0.5, 2.0]
      a_range: [0.1, 0.5]
    test_out:
-     n_samples: 1
+     n_samples: 100   # 增加到100条out-of-distribution测试数据
      v0_range: [2.0, 3.0]
      a_range: [0.5, 1.0]
```

```diff
    save_steps: 100
  
- # # 测试配置
- # testing:
- #   model_checkpoint_path: "checkpoints/best_model"  # 训练好的模型路径
- #   batch_size: 8
- #   num_samples: 100  # 用于可视化的样本数量
- #   visualization:
- #     figure_size: [10, 6]
- #     dpi: 100
+ # 测试配置
+ testing:
+   model_checkpoint_path: "checkpoints/best_model"
+   batch_size: 8
+   num_samples: 100
+   visualization:
+     figure_size: [10, 6]
+     dpi: 100
+ 
+ # 可视化配置
+ visualization:
+   plotting:
+     figure_size: [12, 6]
+     style: "seaborn"
+     dpi: 100
  
  # 奖励函数配置
```

---

使用新的配置，让我们重新计算训练步数：

```
数据集大小 = 1000
batch_size = 1
gradient_accumulation_steps = 4
epochs = 10

总训练步数 = (1000 / (1 * 4)) * 10 = 2500 步
```

这样的训练步数更合理，因为：
1. 每个样本会生成4个回答（num_generations=4），实际上模型会看到 4000 个样本
2. 有足够的步数让模型学习和调整
3. 考虑到保存间隔（save_steps=500），会保存5个检查点，再加上最终模型

建议：
1. 如果训练不稳定，可以：
   - 降低学习率（当前是1e-6，可以试试5e-7）
   - 增加梯度累积步数
   - 增加训练轮数
2. 如果显存允许，可以：
   - 增加 batch_size
   - 增加 num_generations（生成更多样本）
3. 监控训练过程：
   - 使用 swanlab 观察损失和奖励的变化
   - 如果出现 COT 输出乱码，根据作业提示，可以继续训练一段时间看是否能自我调整

您觉得这些设置合适吗？我们可以根据您的具体需求进行调整。

---

_**User**_

# coding=utf-8
# Copyright 2025 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Reward functions for GRPO training."""

import asyncio
import json
import math
import re
from functools import partial, update_wrapper
from typing import Callable, Dict, Literal, Optional

from latex2sympy2_extended import NormalizationConfig
from math_verify import LatexExtractionConfig, parse, verify

from .utils.code_providers import get_provider
from .utils.competitive_programming import (
    SubtaskResult,
    add_includes,
    get_morph_client_from_env,
    get_piston_client_from_env,
)
from .utils.competitive_programming import patch_code as cf_patch_code
from .utils.competitive_programming import score_submission as cf_score_submission
from .utils.competitive_programming import score_subtask


def accuracy_reward(completions: list[list[dict[str, str]]], solution: list[str], **kwargs) -> list[Optional[float]]:
    """Reward function that checks if the completion is the same as the ground truth."""
    contents = [completion[0]["content"] for completion in completions]
    rewards = []
    for content, sol in zip(contents, solution):
        gold_parsed = parse(
            sol,
            extraction_mode="first_match",
        )
        if len(gold_parsed) != 0:
            # We require the answer to be provided in correct latex (no malformed operators)
            answer_parsed = parse(
                content,
                extraction_config=[
                    LatexExtractionConfig(
                        normalization_config=NormalizationConfig(
                            nits=False,
                            malformed_operators=False,
                            basic_latex=True,
                            equations=True,
                            boxed="all",
                            units=True,
                        ),
                        # Ensures that boxed is tried first
                        boxed_match_priority=0,
                        try_extract_without_anchor=False,
                    )
                ],
                extraction_mode="first_match",
            )
            # Compute binary rewards if verifiable, `None` otherwise to skip this example
            try:
                reward = float(verify(gold_parsed, answer_parsed))
            except Exception as e:
                print(f"verify failed: {e}, answer: {answer_parsed}, gold: {gold_parsed}")
                reward = None
        else:
            # If the gold solution is not parseable, we assign `None` to skip this example
            reward = None
            print("Failed to parse gold solution: ", sol)
        rewards.append(reward)

    return rewards


def format_reward(completions, **kwargs):
    """Reward function that checks if the reasoning process is enclosed within <think> and </think> tags, while the final answer is enclosed within <answer> and </answer> tags."""
    pattern = r"^<think>\n.*?\n</think>\n<answer>\n.*?\n</answer>$"
    completion_contents = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, content, re.DOTALL | re.MULTILINE) for content in completion_contents]
    return [1.0 if match else 0.0 for match in matches]


def tag_count_reward(completions, **kwargs) -> list[float]:
    """Reward function that checks if we produce the desired number of think and answer tags associated with `format_reward()`.

    Adapted from: https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb#file-grpo_demo-py-L90
    """

    def count_tags(text: str) -> float:
        count = 0.0
        if text.count("<think>\n") == 1:
            count += 0.25
        if text.count("\n</think>\n") == 1:
            count += 0.25
        if text.count("\n<answer>\n") == 1:
            count += 0.25
        if text.count("\n</answer>") == 1:
            count += 0.25
        return count

    contents = [completion[0]["content"] for completion in completions]
    return [count_tags(c) for c in contents]


def reasoning_steps_reward(completions, **kwargs):
    r"""Reward function that checks for clear step-by-step reasoning.
    Regex pattern:
        Step \d+: - matches "Step 1:", "Step 2:", etc.
        ^\d+\. - matches numbered lists like "1.", "2.", etc. at start of line
        \n- - matches bullet points with hyphens
        \n\* - matches bullet points with asterisks
        First,|Second,|Next,|Finally, - matches transition words
    """
    pattern = r"(Step \d+:|^\d+\.|\n-|\n\*|First,|Second,|Next,|Finally,)"
    completion_contents = [completion[0]["content"] for completion in completions]
    matches = [len(re.findall(pattern, content)) for content in completion_contents]

    # Magic number 3 to encourage 3 steps and more, otherwise partial reward
    return [min(1.0, count / 3) for count in matches]


def len_reward(completions: list[Dict[str, str]], solution: list[str], **kwargs) -> float:
    """Compute length-based rewards to discourage overthinking and promote token efficiency.

    Taken from the Kimi 1.5 tech report: https://huggingface.co/papers/2501.12599

    Args:
        completions: List of model completions
        solution: List of ground truth solutions

    Returns:
        List of rewards where:
        - For correct answers: reward = 0.5 - (len - min_len)/(max_len - min_len)
        - For incorrect answers: reward = min(0, 0.5 - (len - min_len)/(max_len - min_len))
    """
    contents = [completion[0]["content"] for completion in completions]

    # First check correctness of answers
    correctness = []
    for content, sol in zip(contents, solution):
        gold_parsed = parse(
            sol,
            extraction_mode="first_match",
            extraction_config=[LatexExtractionConfig()],
        )
        if len(gold_parsed) == 0:
            # Skip unparseable examples
            correctness.append(True)  # Treat as correct to avoid penalizing
            print("Failed to parse gold solution: ", sol)
            continue

        answer_parsed = parse(
            content,
            extraction_config=[
                LatexExtractionConfig(
                    normalization_config=NormalizationConfig(
                        nits=False,
                        malformed_operators=False,
                        basic_latex=True,
                        equations=True,
                        boxed=True,
                        units=True,
                    ),
                    boxed_match_priority=0,
                    try_extract_without_anchor=False,
                )
            ],
            extraction_mode="first_match",
        )
        correctness.append(verify(answer_parsed, gold_parsed))

    # Calculate lengths
    lengths = [len(content) for content in contents]
    min_len = min(lengths)
    max_len = max(lengths)

    # If all responses have the same length, return zero rewards
    if max_len == min_len:
        return [0.0] * len(completions)

    rewards = []
    for length, is_correct in zip(lengths, correctness):
        lambda_val = 0.5 - (length - min_len) / (max_len - min_len)

        if is_correct:
            reward = lambda_val
        else:
            reward = min(0, lambda_val)

        rewards.append(float(reward))

    return rewards


def get_cosine_scaled_reward(
    min_value_wrong: float = -1.0,
    max_value_wrong: float = -0.5,
    min_value_correct: float = 0.5,
    max_value_correct: float = 1.0,
    max_len: int = 1000,
):
    def cosine_scaled_reward(completions, solution, **kwargs):
        """Reward function that scales based on completion length using a cosine schedule.

        Shorter correct solutions are rewarded more than longer ones.
        Longer incorrect solutions are penalized less than shorter ones.

        Args:
            completions: List of model completions
            solution: List of ground truth solutions

        This function is parameterized by the following arguments:
            min_value_wrong: Minimum reward for wrong answers
            max_value_wrong: Maximum reward for wrong answers
            min_value_correct: Minimum reward for correct answers
            max_value_correct: Maximum reward for correct answers
            max_len: Maximum length for scaling
        """
        contents = [completion[0]["content"] for completion in completions]
        rewards = []

        for content, sol in zip(contents, solution):
            gold_parsed = parse(
                sol,
                extraction_mode="first_match",
                extraction_config=[LatexExtractionConfig()],
            )
            if len(gold_parsed) == 0:
                rewards.append(1.0)  # Skip unparseable examples
                print("Failed to parse gold solution: ", sol)
                continue

            answer_parsed = parse(
                content,
                extraction_config=[
                    LatexExtractionConfig(
                        normalization_config=NormalizationConfig(
                            nits=False,
                            malformed_operators=False,
                            basic_latex=True,
                            equations=True,
                            boxed=True,
                            units=True,
                        ),
                        boxed_match_priority=0,
                        try_extract_without_anchor=False,
                    )
                ],
                extraction_mode="first_match",
            )

            is_correct = verify(answer_parsed, gold_parsed)
            gen_len = len(content)

            # Apply cosine scaling based on length
            progress = gen_len / max_len
            cosine = math.cos(progress * math.pi)

            if is_correct:
                min_value = min_value_correct
                max_value = max_value_correct
            else:
                # Swap min/max for incorrect answers
                min_value = max_value_wrong
                max_value = min_value_wrong

            reward = min_value + 0.5 * (max_value - min_value) * (1.0 + cosine)
            rewards.append(float(reward))

        return rewards

    return cosine_scaled_reward


def get_repetition_penalty_reward(ngram_size: int, max_penalty: float, language: str = "en"):
    """
    Computes N-gram repetition penalty as described in Appendix C.2 of https://huggingface.co/papers/2502.03373.
    Reference implementation from: https://github.com/eddycmu/demystify-long-cot/blob/release/openrlhf/openrlhf/reward/repetition.py

    Args:
    ngram_size: size of the n-grams
    max_penalty: Maximum (negative) penalty for wrong answers
    language: Language of the text, defaults to `en`. Used to choose the way to split the text into n-grams.
    """
    if max_penalty > 0:
        raise ValueError(f"max_penalty {max_penalty} should not be positive")

    if language == "en":

        def zipngram(text: str, ngram_size: int):
            words = text.lower().split()
            return zip(*[words[i:] for i in range(ngram_size)]), words

    elif language == "zh":
        from transformers.utils.import_utils import _is_package_available

        if not _is_package_available("jieba"):
            raise ValueError("Please install jieba to use Chinese language")

        def zipngram(text: str, ngram_size: int):
            import jieba

            seg_list = list(jieba.cut(text))
            return zip(*[seg_list[i:] for i in range(ngram_size)]), seg_list

    else:
        raise ValueError(
            f"Word splitting for language `{language}` is not yet implemented. Please implement your own zip-ngram function."
        )

    def repetition_penalty_reward(completions, **kwargs) -> float:
        """
        reward function the penalizes repetitions
        ref implementation: https://github.com/eddycmu/demystify-long-cot/blob/release/openrlhf/openrlhf/reward/repetition.py

        Args:
            completions: List of model completions
        """

        contents = [completion[0]["content"] for completion in completions]
        rewards = []
        for completion in contents:
            if completion == "":
                rewards.append(0.0)
                continue

            ngrams = set()
            total = 0
            ngram_array, words = zipngram(completion, ngram_size)

            if len(words) < ngram_size:
                rewards.append(0.0)
                continue

            for ng in ngram_array:
                ngrams.add(ng)
                total += 1

            scaling = 1 - len(ngrams) / total
            reward = scaling * max_penalty
            rewards.append(reward)
        return rewards

    return repetition_penalty_reward


def _init_event_loop():
    """Initialize or get the current event loop."""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    return loop


def ioi_code_reward(completions, test_batch_size: int = 1, provider_type: str = "piston", **kwargs) -> list[float]:
    """Reward function that evaluates IOI problems using a specified execution client.

    Assumes the dataset has the same format as hf.co/datasets/open-r1/ioi

    Args:
        completions: List of model completions to evaluate
        test_batch_size: Evaluate these many test cases in parallel, then check if any of them failed (0 score):
                       if so stop evaluating; otherwise continue with the next batch of test cases.
        provider_type: The execution provider to use (default: "piston"). Supported values: "piston", "morph"
        **kwargs: Additional arguments passed from the dataset
    """
    # Get the appropriate client based on provider_type
    if provider_type == "morph":
        execution_client = get_morph_client_from_env()
    else:
        # for info on setting up piston workers, see slurm/piston/README.md
        execution_client = get_piston_client_from_env()

    code_snippets = [
        # note: grading is automatically skipped if no code is extracted
        add_includes(extract_code(completion[-1]["content"], "cpp"), problem_id)
        for completion, problem_id in zip(completions, kwargs["id"])
    ]

    async def run_catch_exceptions(task):
        try:
            return await task
        except Exception as e:
            print(f"Error from {provider_type} worker: {e}")
            return SubtaskResult()

    problems_data = [dict(zip(kwargs.keys(), values)) for values in zip(*kwargs.values())]

    loop = _init_event_loop()
    evals = [
        loop.create_task(
            run_catch_exceptions(
                score_subtask(
                    execution_client,
                    problem_data,
                    code,
                    test_batch_size=test_batch_size,
                )
            )
        )
        for problem_data, code in zip(problems_data, code_snippets)
    ]
    results = loop.run_until_complete(asyncio.gather(*evals))

    return [result.score for result in results]


def cf_code_reward(
    completions,
    test_batch_size: int = 1,
    patch_code: bool = False,
    scoring_mode: Literal["pass_fail", "partial", "weighted_sum"] = "weighted_sum",
    **kwargs,
) -> list[float]:
    """Reward function that evaluates Codeforces problems using Piston+our CF package.

    Assumes the dataset has the same format as hf.co/datasets/open-r1/codeforces (verifiable-prompts subset)

    test_batch_size: evaluate these many test cases in parallel, then check if any of them failed (0 score): if so stop evaluating; otherwise continue with the next batch of test cases.
    """
    # for info on setting up piston workers, see slurm/piston/README.md
    piston_client = get_piston_client_from_env()

    languages = kwargs["language"] if "language" in kwargs else [None] * len(completions)
    code_snippets = [
        # note: grading is automatically skipped if a problem has no tests
        cf_patch_code(extract_code(completion[-1]["content"], language), language)
        if patch_code
        else extract_code(completion[-1]["content"], language)
        for completion, language in zip(completions, languages)
    ]

    async def run_catch_exceptions(task):
        try:
            return await task
        except Exception as e:
            print(f"Error from Piston worker: {e}")
            return None

    # load problem data. undo separating kwargs by column
    problems_data = [dict(zip(kwargs.keys(), values)) for values in zip(*kwargs.values())]

    loop = _init_event_loop()
    evals = [
        loop.create_task(
            run_catch_exceptions(
                cf_score_submission(
                    piston_client,
                    problem_data,
                    code,
                    test_batch_size=test_batch_size,
                    scoring_mode=scoring_mode,
                    submission_language=problem_data.get("language", None),
                )
            )
        )
        for problem_data, code in zip(problems_data, code_snippets)
    ]
    results = loop.run_until_complete(asyncio.gather(*evals))

    return results


def extract_code(completion: str, language: str | None = "python") -> str:
    if language is None:
        return ""
    pattern = re.compile(rf"```{language}\n(.*?)```", re.DOTALL)
    matches = pattern.findall(completion)
    extracted_answer = matches[-1] if len(matches) >= 1 else ""
    return extracted_answer


def binary_code_reward(
    completions,
    num_parallel: int = 2,
    provider_type: str = "e2b",
    enforce_same_language: bool = False,
    **kwargs,
) -> list[float]:
    rewards = code_reward(
        completions,
        num_parallel=num_parallel,
        provider_type=provider_type,
        enforce_same_language=enforce_same_language,
        **kwargs,
    )
    BINARY_THRESHOLD = 0.99

    output = []
    for reward in rewards:
        if reward is None:
            output.append(None)
        else:
            output.append(1.0 if reward > BINARY_THRESHOLD else 0.0)

    return output


def code_reward(
    completions,
    num_parallel: int = 2,
    provider_type: str = "e2b",
    enforce_same_language: bool = False,
    **kwargs,
) -> list[float]:
    """Reward function that evaluates code snippets using a code execution provider.

    Assumes the dataset contains a `verification_info` column with test cases.

    Args:
        completions: List of model completions to evaluate
        num_parallel: Number of parallel code executions (default: 2)
        provider_type: Which code execution provider to use (default: "e2b")
        enforce_same_language: If True, verify all problems use the same language (default: False)
        **kwargs: Additional arguments passed to the verification
    """
    evaluation_script_template = """
    import subprocess
    import json

    def evaluate_code(code, test_cases):
        passed = 0
        total = len(test_cases)
        exec_timeout = 5

        for case in test_cases:
            process = subprocess.run(
                ["python3", "-c", code],
                input=case["input"],
                text=True,
                capture_output=True,
                timeout=exec_timeout
            )

            if process.returncode != 0:  # Error in execution
                continue

            output = process.stdout.strip()

            # TODO: implement a proper validator to compare against ground truth. For now we just check for exact string match on each line of stdout.
            all_correct = True
            for line1, line2 in zip(output.split('\\n'), case['output'].split('\\n')):
                all_correct = all_correct and line1.strip() == line2.strip()

            if all_correct:
                passed += 1

        success_rate = (passed / total)
        return success_rate

    code_snippet = {code}
    test_cases = json.loads({test_cases})

    evaluate_code(code_snippet, test_cases)
    """

    code_snippets = [extract_code(completion[-1]["content"]) for completion in completions]
    verification_info = kwargs["verification_info"]

    template = evaluation_script_template

    scripts = [
        template.format(code=json.dumps(code), test_cases=json.dumps(json.dumps(info["test_cases"])))
        for code, info in zip(code_snippets, verification_info)
    ]

    language = verification_info[0]["language"]

    if enforce_same_language:
        all_same_language = all(v["language"] == language for v in verification_info)
        if not all_same_language:
            raise ValueError("All verification_info must have the same language", verification_info)

    execution_provider = get_provider(
        provider_type=provider_type,
        num_parallel=num_parallel,
        **kwargs,
    )

    return execution_provider.execute_scripts(scripts, ["python"] * len(scripts))


def get_code_format_reward(language: str = "python"):
    """Format reward function specifically for code responses.

    Args:
        language: Programming language supported by E2B https://e2b.dev/docs/code-interpreting/supported-languages
    """

    def code_format_reward(completions, **kwargs):
        # if there is a language field, use it instead of the default language. This way we can have mixed language training.
        languages = kwargs["language"] if "language" in kwargs else [language] * len(completions)

        completion_contents = [completion[0]["content"] for completion in completions]
        matches = [
            re.match(
                rf"^<think>\n.*?\n</think>\n<answer>\n.*?```{sample_language}.*?```.*?\n</answer>$",
                content,
                re.DOTALL | re.MULTILINE,
            )
            for content, sample_language in zip(completion_contents, languages)
        ]
        return [1.0 if match else 0.0 for match in matches]

    return code_format_reward


def get_soft_overlong_punishment(max_completion_len, soft_punish_cache):
    """
    Reward function that penalizes overlong completions. It is used to penalize overlong completions,
    but not to reward shorter completions. Reference: Eq. (13) from the DAPO paper (https://huggingface.co/papers/2503.14476)

    Args:
        max_completion_len: Maximum length of the completion
        soft_punish_cache: Minimum length of the completion. If set to 0, no minimum length is applied.
    """

    def soft_overlong_punishment_reward(completion_ids: list[list[int]], **kwargs) -> list[float]:
        """Reward function that penalizes overlong completions."""
        rewards = []
        for ids in completion_ids:
            completion_length = len(ids)
            if completion_length <= max_completion_len - soft_punish_cache:
                rewards.append(0.0)
            elif max_completion_len - soft_punish_cache < completion_length <= max_completion_len:
                rewards.append((max_completion_len - soft_punish_cache - completion_length) / soft_punish_cache)
            else:
                rewards.append(-1.0)
        return rewards

    return soft_overlong_punishment_reward


def get_reward_funcs(script_args) -> list[Callable]:
    REWARD_FUNCS_REGISTRY = {
        "accuracy": accuracy_reward,
        "format": format_reward,
        "reasoning_steps": reasoning_steps_reward,
        "cosine": get_cosine_scaled_reward(
            min_value_wrong=script_args.cosine_min_value_wrong,
            max_value_wrong=script_args.cosine_max_value_wrong,
            min_value_correct=script_args.cosine_min_value_correct,
            max_value_correct=script_args.cosine_max_value_correct,
            max_len=script_args.cosine_max_len,
        ),
        "repetition_penalty": get_repetition_penalty_reward(
            ngram_size=script_args.repetition_n_grams,
            max_penalty=script_args.repetition_max_penalty,
        ),
        "length": len_reward,
        "code": update_wrapper(
            partial(
                code_reward,
                num_parallel=script_args.parallel_code_exec_per_proc,
                provider_type=script_args.code_provider,
                enforce_same_language=getattr(script_args, "enforce_same_language", False),
            ),
            code_reward,
        ),
        "binary_code": update_wrapper(
            partial(
                binary_code_reward,
                num_parallel=script_args.parallel_code_exec_per_proc,
                provider_type=script_args.code_provider,
                enforce_same_language=getattr(script_args, "enforce_same_language", False),
            ),
            binary_code_reward,
        ),
        "ioi_code": update_wrapper(
            partial(
                ioi_code_reward,
                test_batch_size=script_args.code_eval_test_batch_size,
                provider_type=getattr(script_args, "ioi_provider", "piston"),
            ),
            ioi_code_reward,
        ),
        "cf_code": update_wrapper(
            partial(
                cf_code_reward,
                test_batch_size=script_args.code_eval_test_batch_size,
                scoring_mode=script_args.code_eval_scoring_mode,
            ),
            cf_code_reward,
        ),
        "code_format": get_code_format_reward(language=script_args.code_language),
        "tag_count": tag_count_reward,
        "soft_overlong_punishment": get_soft_overlong_punishment(
            max_completion_len=script_args.max_completion_len,
            soft_punish_cache=script_args.soft_punish_cache,
        ),
    }
    reward_funcs = [REWARD_FUNCS_REGISTRY[func] for func in script_args.reward_funcs]

    return reward_funcs

学习一下开源的open-r1的奖励函数，看看能否借鉴一些，来优化我的这个项目。

---

_**Assistant**_

我来帮您分析一下 open-r1 的奖励函数设计，并结合您的运动预测项目提出优化建议：

从 open-r1 的代码中，我们可以学习到以下几个重要的奖励函数设计思路：

1. 格式奖励（Format Reward）：
```python
def format_reward(completions, **kwargs):
    """检查回答是否符合指定格式，使用<think>和<answer>标签包装思考过程和最终答案"""
    pattern = r"^<think>\n.*?\n</think>\n<answer>\n.*?\n</answer>$"
```

2. 推理步骤奖励（Reasoning Steps Reward）：
```python
def reasoning_steps_reward(completions, **kwargs):
    """检查是否有清晰的步骤标记，如Step 1:, 1., 或First,等过渡词"""
    pattern = r"(Step \d+:|^\d+\.|\n-|\n\*|First,|Second,|Next,|Finally,)"
```

3. 长度奖励（Length Reward）：
```python
def len_reward(completions, solution, **kwargs):
    """基于答案长度的奖励，鼓励简洁且正确的答案"""
    # 正确答案：reward = 0.5 - (len - min_len)/(max_len - min_len)
    # 错误答案：reward = min(0, 0.5 - (len - min_len)/(max_len - min_len))
```

基于这些思路，我建议对您的项目做如下优化：

1. 修改 format_reward 函数，增加更细致的格式评分：

---

```diff
  
  def format_reward(prompts: List[str], completions: List[str], **kwargs) -> List[float]:
-     """Evaluate response format and structure
-     
-     Scoring criteria:
-     1. Basic structure (0.2): [Coordinates] and [Analysis] headers
-     2. Coordinates format (0.3): Exactly 5 coordinates in (x.xx) format
-     3. Analysis steps (0.4): S1-S4 with correct labels
-     4. Overall formatting (0.1): Proper spacing and newlines
+     """评估回答的格式规范性
+     
+     评分标准：
+     1. 基本结构 (0.3): [Coordinates] 和 [Analysis] 标签正确
+     2. 坐标格式 (0.3): 正确的坐标格式 (x.xx)
+     3. 分析步骤 (0.4): 包含 S1-S4 完整的分析步骤
      """
      rewards = []
      
-     # 定义期望的格式模式
+     # 定义格式模式
      patterns = {
          # 基本结构
-         'coords_header': r'\[Coordinates\]',
-         'analysis_header': r'\[Analysis\]',
+         'structure': {
+             'coords_header': r'\[Coordinates\]\n',
+             'analysis_header': r'\[Analysis\]\n',
+             'step_separation': r'\n\n'
+         },
          
          # 坐标格式
-         'coords_format': r'\([-+]?\d+\.\d{2}\)',
+         'coordinates': {
+             'format': r'\(([-+]?\d+\.\d{2})\)',
+             'separation': r',\s*'
+         },
          
          # 分析步骤
-         'step_patterns': {
+         'steps': {
              'S1': r'S1:\s*.*observation',
              'S2': r'S2:\s*.*calculation',
              'S3': r'S3:\s*.*verification',
-             'S4': r'S4:\s*.*prediction',
+             'S4': r'S4:\s*.*prediction'
          }
      }
```

```diff
          score = 0.0
          
-         # 1. 检查基本结构 (0.2分)
-         if re.search(patterns['coords_header'], completion, re.IGNORECASE):
-             score += 0.1
-         if re.search(patterns['analysis_header'], completion, re.IGNORECASE):
-             score += 0.1
+         # 1. 基本结构检查 (0.3分)
+         structure_score = 0.0
+         if re.search(patterns['structure']['coords_header'], completion):
+             structure_score += 0.1
+         if re.search(patterns['structure']['analysis_header'], completion):
+             structure_score += 0.1
+         if re.search(patterns['structure']['step_separation'], completion):
+             structure_score += 0.1
+         score += structure_score
          
-         # 2. 检查坐标格式 (0.3分)
-         coords = re.findall(patterns['coords_format'], completion)
-         if coords:  # 有坐标
+         # 2. 坐标格式检查 (0.3分)
+         coords_score = 0.0
+         coords = re.findall(patterns['coordinates']['format'], completion)
+         if coords:
              if len(coords) == 5:  # 正好5个坐标
-                 score += 0.2
-             # 检查格式是否正确（包括逗号分隔）
-             coords_line = re.search(r'\[Coordinates\].*?\n(.*?)\n', completion, re.DOTALL)
-             if coords_line and re.match(r'\s*\([-+]?\d+\.\d{2}\)(\s*,\s*\([-+]?\d+\.\d{2}\))*\s*$', coords_line.group(1)):
-                 score += 0.1
+                 coords_score += 0.2
+             # 检查分隔符
+             if re.search(patterns['coordinates']['separation'], completion):
+                 coords_score += 0.1
+         score += coords_score
          
-         # 3. 检查分析步骤 (0.4分)
-         for step, pattern in patterns['step_patterns'].items():
-             if re.search(pattern, completion, re.IGNORECASE):
-                 score += 0.1
-                 
-         # 4. 检查整体格式 (0.1分)
-         # 检查是否有适当的空行和缩进
-         if re.search(r'\[Coordinates\]\n.*?\n\n\[Analysis\]', completion, re.DOTALL):
-             score += 0.05
-         # 检查是否以换行符结束
-         if completion.strip().endswith('\n'):
-             score += 0.05
+         # 3. 分析步骤检查 (0.4分)
+         steps_score = 0.0
+         for step_pattern in patterns['steps'].values():
+             if re.search(step_pattern, completion, re.IGNORECASE):
+                 steps_score += 0.1
+         score += steps_score
          
-         rewards.append(float(score))  # 确保返回Python float
+         rewards.append(float(score))
      
      return rewards
```

---

2. 添加一个新的 reasoning_quality_reward 函数来评估推理质量：

---

```diff
      return rewards
  
+ 
+ def reasoning_quality_reward(prompts: List[str], completions: List[str], **kwargs) -> List[float]:
+     """评估推理过程的质量
+     
+     评分标准：
+     1. 观察完整性 (0.3): 是否正确识别数列特征
+     2. 计算准确性 (0.3): 是否包含具体的数值计算
+     3. 预测方法 (0.4): 是否清晰说明预测方法
+     """
+     rewards = []
+     
+     patterns = {
+         # 观察完整性
+         'observation': {
+             'pattern': r'(增加|减少|变化|规律|间隔|等差|等比)',
+             'numbers': r'[-+]?\d+\.\d{2}',
+             'comparison': r'(大于|小于|等于|差|比)'
+         },
+         
+         # 计算准确性
+         'calculation': {
+             'arithmetic': r'[-+]?\d+\.\d{2}\s*[-+*/]\s*[-+]?\d+\.\d{2}\s*=\s*[-+]?\d+\.\d{2}',
+             'difference': r'差(值|是|为)?\s*[-+]?\d+\.\d{2}',
+             'steps': r'(第一步|第二步|首先|然后|接着|最后)'
+         },
+         
+         # 预测方法
+         'prediction': {
+             'method': r'(通过|使用|根据|基于)',
+             'formula': r'(公式|表达式|方程|函数)',
+             'verification': r'(验证|检查|确认|证明)'
+         }
+     }
+     
+     for completion in completions:
+         score = 0.0
+         
+         # 1. 观察完整性评分 (0.3)
+         observation_score = 0.0
+         if re.search(patterns['observation']['pattern'], completion):
+             observation_score += 0.1
+         if re.search(patterns['observation']['numbers'], completion):
+             observation_score += 0.1
+         if re.search(patterns['observation']['comparison'], completion):
+             observation_score += 0.1
+         score += observation_score
+         
+         # 2. 计算准确性评分 (0.3)
+         calculation_score = 0.0
+         if re.search(patterns['calculation']['arithmetic'], completion):
+             calculation_score += 0.1
+         if re.search(patterns['calculation']['difference'], completion):
+             calculation_score += 0.1
+         if re.search(patterns['calculation']['steps'], completion):
+             calculation_score += 0.1
+         score += calculation_score
+         
+         # 3. 预测方法评分 (0.4)
+         prediction_score = 0.0
+         if re.search(patterns['prediction']['method'], completion):
+             prediction_score += 0.15
+         if re.search(patterns['prediction']['formula'], completion):
+             prediction_score += 0.15
+         if re.search(patterns['prediction']['verification'], completion):
+             prediction_score += 0.1
+         score += prediction_score
+         
+         rewards.append(float(score))
+     
+     return rewards
+ 
  
  # def format_reward(prompts: List[str], completions: List[str], **kwargs) -> List[float]:
```

---



---

