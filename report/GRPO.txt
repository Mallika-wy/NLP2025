GRPO强化学习算法详解：从基础到大模型应用


强化学习（Reinforcement Learning, RL）作为机器学习的一个重要分支，近年来在人工智能领域取得了突破性进展。它使智能体能够在复杂、动态的环境中自主学习最优行为，而无需明确的编程指令。本报告旨在为强化学习初学者提供一个全面且深入的GRPO（Group Relative Policy Optimization）算法解析，涵盖其基础理论、发展历程，并重点阐述其在大规模语言模型（LLM）训练中的独特优势与应用。


1. 引言：强化学习基础概念


强化学习的核心在于智能体（Agent）与环境（Environment）的持续交互。智能体通过执行动作（Action）来影响环境，环境则根据这些动作产生新的状态（State）并提供奖励（Reward）。智能体的最终目标是学习一个策略（Policy），使其能够最大化长期累积的奖励。这种学习方式与传统的监督学习截然不同，因为它不依赖于预先标注的数据集，而是通过反复的试错和从环境中获得的间接反馈来优化自身行为 1。
强化学习的学习过程与人类或动物的学习行为具有相似之处。例如，儿童通过父母的表扬和批评来理解哪些行为是可取的，哪些是不可取的。积极的反馈（奖励）会鼓励某种行为的重复，而消极的反馈（惩罚）则会抑制不当行为。这种通过间接反馈来调整行为的机制，使得强化学习特别适用于那些规则复杂、难以通过显式编程来解决的问题，例如游戏、机器人控制等。它揭示了智能体如何在没有明确指令的情况下，自主探索并发现最优行为的强大潜力，能够像人类一样从经验中学习和适应。


1.1 强化学习的核心要素：智能体、环境、状态、动作、奖励


强化学习系统由几个关键组成部分构成，它们共同定义了智能体学习和决策的框架：
* 智能体 (Agent): 智能体是强化学习中的“学习者”或“决策者”。它负责观察环境、选择并执行动作，并根据环境的反馈来调整其决策规则 3。
* 环境 (Environment): 环境是智能体所处的外部世界，包含了智能体进行交互所需的所有变量、边界值、规则和有效操作。环境接收智能体的动作，并以新的状态和奖励作为响应 3。
* 状态 (State, s): 状态是对环境在特定时间点的一个完整描述。智能体根据当前状态来决定下一步的动作。例如，在游戏中，屏幕上的画面可以被视为当前状态 3。
* 动作 (Action, a): 动作是智能体在给定状态下可以执行的操作。动作可以是离散的，例如在简单游戏中向左或向右移动；也可以是连续的，例如机器人手臂的精确角度调整 1。
* 奖励 (Reward, r): 奖励是智能体执行某个动作后从环境获得的即时反馈，可以是正值（鼓励）、负值（惩罚）或零值。智能体的终极目标是最大化其在长时间内获得的累积奖励 1。
* 累积奖励 (Cumulative Reward): 累积奖励是智能体从当前时间步开始，到未来所有时间步所获得奖励的总和。在计算累积奖励时，通常会引入一个折扣系数 (γ)。这个折扣系数用于衡量未来奖励的当前价值，即距离当前时间步越远的奖励，对当前决策的影响越小 2。
在强化学习中，一个重要的考量是近期奖励和远期奖励之间的权衡。智能体不能仅仅追求眼前的即时奖励，而必须考虑其行为对未来可能产生的长期影响。折扣系数 γ 的引入，正是为了在数学上量化这种权衡，促使智能体学习那些能够带来更大长期回报的策略，即使这些回报可能需要经过多个时间步才能显现 2。这种对未来回报的考量，是强化学习与简单贪婪算法的重要区别。


1.2 马尔可夫决策过程 (MDP) 简介


马尔可夫决策过程（Markov Decision Process, MDP）是强化学习的数学框架，几乎所有的强化学习问题都可以被建模成MDP。它提供了一种形式化的方式来描述智能体与环境的交互 2。
一个MDP通常由一个五元组 (S,A,P,R,γ) 表示：
* S (状态空间): 所有可能状态的集合 2。
* A (动作空间): 智能体在环境中所有能够选择的动作的集合 2。
* P(s′∣s,a) (状态转移概率): 在状态 s 执行动作 a 后，环境转移到新状态 s′ 的概率 2。
* R(s,a) 或 R(s) (奖励函数): 在状态 s 执行动作 a 后获得的奖励 2。
* γ (折扣系数): 衡量未来奖励的当前价值，通常介于0和1之间 2。
MDP的核心是其“马尔可夫性”，即当前状态包含了做出最优决策所需的所有信息，未来的状态只取决于当前状态和当前动作，与过去的历史无关。在实际应用中，环境的状态转移概率 P 往往是未知的。这种情况下，智能体无法直接构建环境的完整模型，因此需要采用“无模型强化学习”方法。这意味着智能体不能简单地通过规划来找到最优路径，而必须通过大量的试错来直接从经验中学习。这种对无模型学习的需求直接催生了策略梯度和价值函数等方法，它们能够在环境模型未知的情况下，依然有效地让智能体学习如何行动，这也是强化学习与传统控制理论的一个重要区别 1。


1.3 策略与价值函数：智能体决策的基石


为了在MDP中找到最优行为，强化学习智能体主要依赖于两个核心概念：
* 策略 (Policy, π): 策略是智能体用于选择下一步动作的规则或函数。它定义了在给定状态下，智能体应该采取什么行动。策略可以是确定性策略（在同一状态下总是采取相同动作）或随机性策略（根据概率分布选择动作）。随机性策略的优点包括：有助于探索环境、动作具有多样性，并在多智能体博弈中不易被对手预测 2。
* 价值函数 (Value Function): 价值函数用于评估在特定策略下或最优策略下，某个状态或某个状态-动作对的长期价值（即预期累积奖励）。价值函数值越大，说明智能体进入该状态或采取该动作越有利 3。
   * 状态-价值函数 (Vπ(s)): 在遵循特定策略 π 的情况下，从状态 s 开始，智能体预期能获得的累积奖励 6。
   * 动作-价值函数 (Qπ(s,a)): 在状态 s 采取动作 a，然后遵循策略 π 所能获得的预期累积奖励 2。
   * 最优策略下的价值函数 (V∗(s), Q∗(s,a)): 表示在所有可能的策略中，从某个状态或状态-动作对开始所能获得的最大累积奖励 2。
策略与价值函数代表了强化学习算法设计的两种主要思路。基于价值的智能体（如Q-learning, DQN）显式地学习价值函数，然后从学到的价值函数中推算出策略。而基于策略的智能体则直接学习策略，给定一个状态，它就会输出对应动作的概率，不一定学习价值函数。这两种方法的结合产生了“演员-评论员（Actor-Critic）”智能体，它利用价值函数（评论员）来评估策略（演员）的动作，从而更稳定、高效地学习 3。这种内在联系和分工是理解后续策略优化算法（如TRPO, PPO, GRPO）的基础，因为它们多属于策略梯度或Actor-Critic范畴。


1.4 强化学习的交互与学习机制


强化学习是一个迭代的过程，智能体通过反复试错来不断学习和优化其行为：
1. 初始化: 智能体在环境中以初始状态启动 1。
2. 动作选择: 基于当前状态，智能体根据其决策策略选择一个动作 1。
3. 交互: 智能体在环境中执行所选动作 1。
4. 环境响应: 环境以新的状态和奖励作为响应，指示动作的结果 1。
5. 积累经验: 智能体将状态、动作、奖励和新状态等信息记录下来，形成“轨迹”（或称“经验”）。这些轨迹是智能体学习的宝贵数据 1。
6. 学习与更新: 智能体通过优化过程，利用积累的轨迹信息更新其策略（或价值函数）。这可以通过无模型方法（如策略梯度、基于价值、演员-评论家）或基于模型方法（智能体学习环境模型）来执行 1。
7. 重复: 该过程重复进行，使智能体能够通过反复试错不断学习并优化其行为 1。
在整个学习过程中，智能体面临一个基本困境：是应该进一步探索环境以学习新的状态-动作-奖励关系（探索），还是应该从已知能带来高奖励的动作中选择（利用） 4？过度探索会导致效率低下，可能无法收敛到最优解；而过度利用则可能使智能体陷入局部最优，错过全局最优解。这种探索与利用的权衡是设计有效强化学习算法的关键挑战之一，许多算法都包含了解决这一问题的机制，例如随机性策略中的随机性，或Q-learning中的$\epsilon$-greedy策略。理解这一权衡对于理解后续算法如何平衡学习效率和收敛性至关重要。
表1：强化学习核心概念一览


概念 (Concept)
	定义 (Definition)
	作用/目的 (Role/Purpose)
	智能体 (Agent)
	强化学习中的学习者和决策者。
	观察环境，选择并执行动作，优化自身行为以最大化累积奖励。
	环境 (Environment)
	智能体所处的外部世界，接收动作并给出响应。
	提供状态和奖励反馈，定义问题空间。
	状态 (State, s)
	环境在特定时间点的描述。
	智能体决策的依据。
	动作 (Action, a)
	智能体在给定状态下可以执行的操作。
	智能体影响环境的方式。
	奖励 (Reward, r)
	智能体执行动作后获得的即时反馈。
	指导智能体学习的信号，目标是最大化其累积值。
	累积奖励 (Cumulative Reward)
	从当前时刻起未来所有奖励的总和（考虑折扣）。
	智能体优化的最终目标，衡量长期表现。
	马尔可夫决策过程 (MDP)
	强化学习的数学框架，由$(S, A, P, R, \gamma)$组成。
	形式化描述智能体与环境的交互过程。
	策略 (Policy, π)
	智能体选择动作的规则或函数。
	决定智能体在给定状态下如何行动，是强化学习算法学习的目标。
	价值函数 (Value Function)
	评估状态或状态-动作对的长期价值（预期累积奖励）。
	帮助智能体评估当前决策的长期影响，指导策略优化。
	

2. 策略梯度方法：从REINFORCE到挑战


在强化学习中，策略梯度方法是一种直接优化智能体策略的强大工具。


2.1 策略梯度：直接优化策略


与基于价值的方法（如Q-learning，DQN）不同，策略梯度方法不通过学习价值函数来间接推导策略，而是直接学习并优化策略函数本身。这意味着，策略梯度方法会直接调整策略的参数，使其在给定状态下输出动作的概率分布，旨在最大化智能体在环境中获得的期望累积奖励 8。
策略通常被参数化，例如使用神经网络来建模。神经网络的权重即为策略的参数 θ。目标是找到最优的参数 θ，使得策略能够引导智能体做出最佳决策。策略梯度定理是这类方法的核心，它提供了一种计算期望回报对策略参数梯度的有效方法，使得智能体可以沿着梯度方向调整策略，从而逐步提高性能 8。
策略梯度方法在处理连续动作空间问题时具有显著优势 2。基于价值的算法（如DQN）在离散动作空间中表现出色，但当动作空间是连续时，其贪婪策略需要在每个时间步进行复杂的优化，这通常速度太慢，且无法应用于大型无约束的函数优化器。相比之下，策略梯度算法直接将策略近似，因此能够很好地在连续空间中对动作进行搜索，这对于机器人控制等需要精确连续动作的现实世界应用至关重要 1。


2.2 REINFORCE算法：策略梯度的先驱


REINFORCE算法是策略梯度方法中的一个早期且基础的代表。它是一种蒙特卡洛策略梯度方法，通过采样完整的轨迹（即从开始状态到终止状态的一系列状态、动作和奖励）来估计动作的价值，并利用这些估计值来计算策略梯度并更新策略 8。
REINFORCE算法的具体流程如下：
1. 初始化策略参数 θ。
2. 循环迭代：
   * 使用当前策略 πθ​ 与环境交互，采样得到一条完整的轨迹。
   * 计算该轨迹中每个时间步之后所获得的累积回报 Gt​。
   * 根据策略梯度公式，利用 Gt​ 来更新策略参数 θ。这个更新通常通过梯度上升实现，即 θ←θ+α∇θ​J(θ)，其中 α 是学习率 8。


2.3 策略梯度方法的挑战：高方差与不稳定性


尽管REINFORCE算法简单直观，但它在实际应用中面临一些显著挑战：
* 高方差 (High Variance): REINFORCE算法的梯度估计方差很大 8。这是因为每次更新都依赖于一条完整的采样轨迹的回报值，而这些回报值在不同轨迹之间波动较大，导致梯度估计不够稳定，进而影响训练效果。
* 步长选择敏感性 (Step Size Sensitivity): 策略梯度算法在更新参数时，学习率（步长）的选择至关重要。如果步长设置得太长，策略性能可能突然显著变差，甚至导致训练崩溃。反之，步长太短则会使学习过程非常缓慢 14。
* 在线策略 (On-policy): REINFORCE是一种在线策略算法 2。这意味着它必须使用当前策略与环境交互采样得到的数据来计算梯度并更新策略。一旦策略更新，之前收集到的轨迹数据就不能被再次利用，这导致样本效率低下 8。低样本效率直接意味着智能体需要与环境进行大量的交互才能有效学习，这在许多复杂或真实的场景中（例如机器人学习）会带来巨大的计算成本和时间消耗。
这些挑战促使研究人员寻求更稳定、更高效的策略优化算法，从而引出了信任区域优化方法的诞生。


3. 信任区域优化：TRPO与PPO


为了解决策略梯度方法中训练不稳定和步长选择敏感的问题，研究人员引入了“信任区域”的概念，旨在通过限制策略更新的幅度来保证学习过程的稳定性。


3.1 TRPO (Trust Region Policy Optimization)：稳定性的追求


动机: 信任区域策略优化（TRPO）算法在2015年被提出，其核心动机是针对传统策略梯度算法训练不稳定、步长选择敏感的缺点，旨在理论上保证策略性能的单调性提升 11。
信任区域的概念与作用:
在优化过程中，一个“信任区域”可以被理解为类似于最大步长的概念。与传统的梯度下降方法（先找到一个改进方向，再选择一个合适的步长）不同，信任区域方法首先定义一个“信任区域”，然后在这个区域内寻找一个能够改进策略的点。其核心思想是在当前策略周围定义一个“可信赖”的区域，在这个区域内，我们可以认为用来近似真实目标函数的模型是准确的。通过将策略更新限制在这个区域内，TRPO能够防止策略突然显著变差，从而显著提高训练稳定性 15。
KL散度：约束策略更新的关键:
TRPO使用Kullback-Leibler (KL) 散度来量化新旧策略之间的差异，并将其作为约束条件，确保策略更新的平滑性和稳定性 15。KL散度是一种衡量两个概率分布之间距离（尽管不是严格意义上的距离）的方法。TRPO的优化目标是在最大化期望累积奖励的同时，限制新策略与旧策略的KL散度不超过预设的阈值
δ。这一约束确保了新策略不会在参数空间中离旧策略太远，从而避免了性能的剧烈下降，并保证了策略的单调改进 15。
TRPO的优缺点与计算复杂度:
* 优点: TRPO在理论上能够保证策略性能的单调性提升，并在实际应用中取得了比传统策略梯度算法更好的效果，尤其在复杂环境中表现出良好的稳定性 11。
* 缺点: TRPO的主要缺点是其计算复杂度高 12。它需要计算二阶导数（通过Fisher信息矩阵近似自然梯度）并执行共轭梯度下降来求解优化问题。这种复杂的计算过程使得TRPO对于大型模型（如深度神经网络）来说，在实际应用中变得不切实际，因为其计算量和内存消耗巨大。这种算法的复杂性与实际应用之间的权衡，是推动后续算法发展的重要因素。


3.2 PPO (Proximal Policy Optimization)：效率与实用性的平衡


PPO如何简化TRPO: 近端策略优化（PPO）算法在2017年被提出，旨在提供TRPO的稳定性优势，同时显著降低计算复杂度，实现效率与实用性的平衡。PPO是TRPO的简化版本，它通过使用“截断替代目标函数”来限制策略变化，而不是TRPO那种计算复杂的硬性KL散度约束 12。
截断替代目标函数 (Clipped Surrogate Objective Function):
PPO的核心创新在于其独特的截断机制。它在目标函数中引入了一个截断项，限制新旧策略概率比率在一个预设的小范围 (ϵ) 内。如果概率比率超出这个范围，它就会被截断到边界值。这种机制间接限制了策略更新的幅度，从而避免了策略的剧烈变化 13。这种方法使得优化问题变得更简单，可以使用一阶优化方法（如梯度下降）求解，大大提高了计算效率，使其比TRPO更易于实现和扩展 17。
PPO的优势与应用:
* 优点: PPO易于实现，计算高效，在多种强化学习任务中表现良好，并且训练过程通常更稳定，收敛速度更快，样本利用率更高 12。由于其出色的性能和实用性，PPO已成为OpenAI默认的强化学习算法，并被广泛应用于大型语言模型（LLM）的训练，例如ChatGPT、Gemini等 13。
* 缺点: 尽管PPO简化了TRPO，但在某些敏感场景下仍可能出现不稳定性，并且其性能对超参数的选择较为敏感 12。
PPO的出现代表了强化学习算法发展中的一个重要趋势：从追求纯粹的理论严谨性（如TRPO）转向更注重实用性和效率。PPO证明了即使牺牲一些理论上的严格保证，通过巧妙的近似和简化，也能获得在实际应用中表现卓越的算法。这种对实用性的重视，为后续GRPO等算法的诞生奠定了基础，特别是针对大规模模型训练的需求。
表2：策略优化算法对比 (TRPO, PPO, GRPO)


算法 (Algorithm)
	策略更新机制 (Policy Update Mechanism)
	稳定性 (Stability)
	计算复杂度 (Computational Complexity)
	内存占用 (Memory Usage)
	优点 (Advantages)
	缺点 (Disadvantages)
	典型应用 (Typical Applications)
	TRPO
	KL散度硬性约束，二阶近似
	理论保证单调提升，高稳定性
	高（二阶导数，共轭梯度）
	中等
	理论保障，单调改进
	复杂，计算慢，不适合大模型
	机器人控制
	PPO
	截断替代目标，限制概率比率
	良好稳定性，但敏感场景可能不稳定
	低（一阶优化）
	高（需要价值模型）
	效率高，易实现，性能好，广泛应用
	对超参数敏感，敏感场景可能不稳定
	通用RL任务，LLM训练 (ChatGPT, Gemini)
	GRPO
	组内相对优势，KL散度惩罚，移除价值模型
	良好稳定性，通过数据分组和奖励归一化缓解方差
	低（移除价值模型，内存高效）
	低（无需价值模型，适合大模型）
	内存高效，适用于LLM，简化RLHF，在推理任务中表现出色
	移除价值模型可能增加方差，需要精细调优，对长轨迹可能OOM，奖励模型潜在问题
	LLM推理任务微调 (DeepSeekMath, DeepSeek-R1)
	

4. GRPO：面向大模型的创新


尽管PPO在大型语言模型（LLM）的训练中取得了巨大成功，但它仍然面临效率问题，特别是需要训练和维护一个独立的奖励模型和价值模型，这对于参数量庞大的LLM来说，带来了显著的计算和内存开销 13。为了解决这些挑战，研究人员提出了GRPO（Group Relative Policy Optimization）算法。


4.1 GRPO的诞生背景与核心思想


GRPO是PPO的进一步演进，专门为大型语言模型（LLM）的微调而设计。其核心思想是通过引入数据分组（即对相似任务进行聚类）和组内相对优势计算，并移除对独立价值函数的依赖，从而简化RLHF（人类反馈强化学习）流程，大幅提高训练效率、稳定性和内存效率 12。
PPO在训练时通常需要维护多个模型，包括策略模型、奖励模型和价值模型。这些模型都需要进行反向传播，消耗大量内存和计算资源。GRPO的出现，正是为了解决LLM训练中因模型规模巨大而导致的内存和计算瓶颈。通过简化模型架构和优化优势计算方式，GRPO能够显著降低训练成本，使其在资源受限的环境下也能高效训练超大规模模型。


4.2 GRPO如何移除价值函数


传统PPO通常训练一个独立的价值模型来估计动作的价值（即预期累积奖励），并将其作为优势函数（Advantage Function）的基线。这个价值模型需要单独训练，并占用大量内存。GRPO的突破性创新在于它移除了对独立价值函数的依赖 23。
GRPO通过生成多组响应来取代价值模型的功能。具体来说，对于每个给定的查询（prompt），模型会生成一组（例如G个）不同的响应。然后，一个预训练好的奖励模型会对这些响应进行评分。


4.3 组内相对优势计算：Z-score方法


GRPO的核心优势计算方法是基于组内相对差异。对于每个查询 sj​，模型会生成一组 Kj​ 个响应 ajk​（其中 k 从1到 Kj​）。每个响应 ajk​ 会通过一个奖励模型获得一个奖励值 Rjk​ 27。
GRPO随后会计算该组所有响应的平均奖励 Rˉj​=Kj​1​∑k=1Kj​​Rjk​ 27。每个独立响应的优势（Advantage）
Ajk​ 则被计算为该响应的奖励与该组平均奖励之差：Ajk​=Rjk​−Rˉj​ 27。
这种组内相对优势的计算方式，巧妙地通过经验数据为优势函数提供了一个动态的、局部的基线。它类似于使用Z-score（A(s,a)=(ri​−μ)/σ）来标准化奖励，将任意奖励值转化为一个可学习的正负信号，反映了特定响应偏离该组平均表现的标准差数量 23。这种方法能够自然地降低优势估计的方差，提高训练的稳定性。通过比较同一查询下不同响应的相对好坏，GRPO能够更有效地评估动作的质量，而无需一个复杂的独立价值网络来预测未来的累积奖励。这充分利用了LLM生成多样化输出的能力，从而在不增加额外模型负担的情况下，获得了高质量的优势估计。


4.4 GRPO的数学公式与策略更新


GRPO的策略更新目标函数与PPO类似，但其优势计算方式有所不同。它包含两部分：最大化优势项和KL散度惩罚项 27。
损失函数形式如下：
L=−∑j=1M​∑k=1Kj​​(πθold​​(ajk​∣sj​)πθ​(ajk​∣sj​)​Ajk​)+β∑j=1M​KL(πθ​(⋅∣sj​)∣∣πθold​​(⋅∣sj​)) 27
其中：
* M 代表总的查询数量。
* πθ​ 是新策略，由参数 θ 参数化。
* πθold​​ 是旧策略。
* πθold​​(ajk​∣sj​)πθ​(ajk​∣sj​)​ 是重要性采样比率，用于校正新旧策略之间的数据分布差异。对于一个序列 ajk​，该比率是序列中每个token比率的乘积 27。
* Ajk​ 是前面介绍的组内相对优势。
* β 是一个系数，用于控制KL散度惩罚项的强度。这个惩罚项确保新策略不过度偏离旧策略，从而维持训练稳定性 23。
KL散度惩罚项在GRPO中扮演着双重角色。首先，它像TRPO和PPO一样，防止策略发生剧烈变化，从而确保训练过程的平滑和稳定，尤其是在移除了价值函数可能导致优势估计噪声增加的情况下 12。其次，对于LLM的微调而言，它还承担着
对齐的重要功能。该项有助于将强化学习过程中的策略更新与模型在监督微调（SFT）阶段学到的初始行为保持一致 23。这可以防止模型在优化特定奖励时“漂移”太远，从而在提升特定任务性能的同时，保持其通用的语言理解和生成能力。


4.5 GRPO的理论优势与实际应用


GRPO的设计带来了多项显著优势，使其成为LLM训练的有力工具：
* 内存效率与可扩展性: 通过移除独立的价值函数，GRPO显著降低了训练所需的内存和计算成本。这使得它特别适合在资源受限的硬件上训练和微调参数量巨大的LLM，极大地提高了大型模型训练的可扩展性 25。
* 改进的决策与泛化能力: 组相对优势的计算方法允许更细致和复杂的策略更新。通过考虑组内不同响应的相对表现，GRPO能够使AI模型发展出更复杂和适应性强的推理能力，更好地理解上下文并实现更强的泛化 28。
* 在LLM推理任务中的表现: GRPO及其变体（如Critique-GRPO）在LLM的复杂推理任务中表现出色。实验表明，Critique-GRPO在数学、STEM（科学、技术、工程、数学）和通用推理任务中，平均pass@1分数提升了约4.5%至6.5%，显著优于传统的监督学习和基于RL的微调方法 30。
* 应用案例：DeepSeekMath与DeepSeek-R1: GRPO算法最初在DeepSeekMath论文中被提出，并成功应用于训练DeepSeek-R1和DeepSeek-V2/V3等模型。这些模型在复杂推理任务中展现出与顶级AI模型竞争的卓越性能，证明了GRPO在实际应用中的有效性 12。


4.6 GRPO的局限性与未来发展


尽管GRPO带来了显著的创新和优势，但它并非没有局限性，并且研究人员正在积极探索其未来的发展方向：
* 局限性:
   * 优势估计方差增加: 移除独立的价值函数虽然节省了内存，但可能导致优势估计的噪声（方差）增加。这需要通过更精细的调优，例如奖励归一化和数据分组等技术来缓解 12。
   * 内存不足（OOM）问题: GRPO需要为每个prompt生成多个样本。如果推理轨迹很长，这可能导致GPU内存不足（Out-Of-Memory, OOM）问题。因此，在实际部署时，需要仔细选择超参数和并行设置以确保训练顺利进行 29。
   * 奖励模型潜在问题: 如果用于评估响应的奖励模型存在系统性错误或对不同对齐方面（如帮助性与安全性）的权重设置不当，策略可能会利用这些错误，导致“奖励欺骗”（reward hacking）——即模型生成高奖励但实际质量低下的输出，或在不同目标之间做出不期望的权衡 35。
* 未来发展:
   * 多模态推理和持续学习: 未来的研究方向包括将GRPO的能力扩展到多模态推理任务和持续学习场景，使其能够处理更复杂的数据类型和动态变化的环境 30。
   * 混合GRPO（Hybrid GRPO）: 研究人员正在探索混合GRPO，它结合了经验多样本动作评估和基于价值函数的引导式价值估计，旨在进一步提高样本效率和学习稳定性 36。
   * 离策略（Off-policy）GRPO: 探索离策略GRPO也是一个重要方向，它允许算法利用旧策略生成的数据进行学习，从而进一步提高样本效率和训练稳定性 32。
   * 更鲁棒的KL散度估计器: 针对实际训练中KL散度估计的挑战，研究更鲁棒的估计方法也是一个潜在的改进点 29。
GRPO的核心创新（通过移除价值模型来提高效率）虽然带来了新的挑战（如优势估计方差），但这些挑战本身也成为了推动新一轮算法创新的动力。这表明强化学习领域的进步是一个持续迭代的过程：解决一个问题往往会引出新的问题，而这些新问题又会驱动下一阶段的算法发展。GRPO并非终极解决方案，而是强化学习在平衡效率、稳定性和性能方面不断演进的重要一步。


5. 总结与展望


本报告从强化学习的基础概念出发，逐步深入探讨了策略优化算法的演进历程。我们看到了从最初的REINFORCE算法如何因高方差和样本效率问题而面临挑战，进而引出了信任区域优化方法。TRPO通过严格的KL散度约束提供了理论上的单调改进保证，但其高昂的计算成本限制了实际应用。PPO则通过引入截断替代目标函数，在TRPO的稳定性基础上实现了计算效率的显著提升，成为通用强化学习任务和LLM训练的主流算法。
在此基础上，GRPO作为PPO的进一步创新，专门为解决大型语言模型训练中的内存和计算瓶颈而设计。GRPO的核心突破在于移除了对独立价值函数的依赖，转而采用组内相对优势计算（通过对同一查询生成多组响应并计算其奖励的相对差异）来估计优势。同时，它保留了KL散度惩罚项，以确保训练的稳定性并维持与监督微调阶段学到的初始行为的对齐。这些创新使得GRPO在内存效率和计算效率上取得了显著优势，极大地简化了LLM的RLHF（人类反馈强化学习）流程，使其能够在有限资源下高效训练超大规模模型。DeepSeekMath和DeepSeek-R1等模型在复杂推理任务中的卓越表现，充分证明了GRPO在LLM领域的强大应用潜力。
然而，GRPO的创新也带来了新的挑战，例如优势估计方差的潜在增加以及对长轨迹处理的内存限制。这些问题正在推动研究人员探索新的解决方案，例如混合GRPO、离策略GRPO以及更鲁棒的KL散度估计方法等。这表明LLM与强化学习的深度融合是一个持续发展的趋势，GRPO是这一融合过程中的一个重要里程碑，预示着未来LLM将通过更先进的强化学习技术实现更强大的能力、更高的可靠性和更好的用户对齐。


6. GRPO强化学习算法展示文稿


（时长：3-5分钟）
________________
[幻灯片1: 标题页]


GRPO强化学习算法详解：大模型训练的效率与创新


[幻灯片2: 开场白 - 约0.5分钟]
大家好！
今天，我将为大家介绍强化学习领域的一个前沿算法：GRPO。
即使您是强化学习的初学者，也请放心，我们将从最基础的概念开始，一步步揭开GRPO的神秘面纱。
我们的目标是理解GRPO如何帮助大型AI模型变得更智能、更高效。
[幻灯片3: 强化学习基础回顾 - 约1分钟]


强化学习基础：智能体如何学习？


* 什么是强化学习？ 想象一下，一个孩子通过父母的表扬和批评来学习行为。强化学习也是如此：智能体通过与环境交互，不断试错，并从环境的“奖励”和“惩罚”中学习，最终目标是最大化长期累积的奖励。 1
* 核心要素：
   * 智能体 (Agent): 做出决策的学习者。
   * 环境 (Environment): 智能体所处的外部世界，提供反馈。
   * 状态 (State): 环境在某一时刻的快照。
   * 动作 (Action): 智能体在状态下采取的行动。
   * 奖励 (Reward): 动作的即时反馈，可以是正或负。
* 交互循环： 智能体观察状态 -> 选择动作 -> 环境给出新状态和奖励 -> 智能体学习并重复。 1
* MDP： 强化学习的数学框架，形式化了这种交互。
* 策略与价值函数：
   * 策略： 智能体行动的“指南”，决定在特定状态下做什么。
   * 价值函数： 评估状态或动作“好坏”的标准，衡量长期预期回报。它们是智能体决策的基石。 3
[幻灯片4: 策略梯度与稳定性挑战 - 约0.5分钟]


策略梯度：直接优化行为，但面临挑战


* 策略梯度方法： 这类方法直接优化智能体的行为策略，而不是先评估价值。这在动作空间连续、复杂的情况下特别有用，例如机器人控制。 2
* REINFORCE算法： 策略梯度的早期代表。它简单直观，但面临两大问题：
   * 高方差： 梯度估计不稳定，训练效果波动大。 8
   * 样本效率低： 每次更新都需要新的数据，旧数据无法重复利用。 8
* 挑战： 训练过程不稳定，学习率（步长）难以选择，可能导致策略性能突然大幅下降。 15
________________


信任区域优化：从TRPO到PPO


* TRPO（信任区域策略优化）：
   * 动机： 为解决策略梯度不稳定性。
   * 核心： 引入“信任区域”概念，通过KL散度（衡量策略差异）约束新旧策略的更新幅度。理论上保证策略性能单调提升。 15
   * 优点： 稳定性强，有理论保障。
   * 缺点： 计算复杂，效率低，不适合处理大规模模型。 12
* PPO（近端策略优化）：
   * 如何简化TRPO： PPO是TRPO的简化和改进。它用一个更简单的“截断替代目标函数”来限制策略更新，避免了TRPO复杂的二阶计算。 12
   * 优点： 易于实现，计算高效，性能良好，成为当前LLM训练的主流算法（如ChatGPT）。 17
   * 缺点： 仍需独立的价值模型，对LLM训练仍有内存和计算开销。 25
________________


GRPO：面向大模型的创新与突破


* 背景： 即使PPO很高效，但对于参数量巨大的LLM，其独立的价值模型仍带来内存和计算瓶颈。 13
* GRPO的突破： 专门为LLM设计，核心创新是移除独立的价值函数！ 23
* 如何实现？
   1. 组内相对优势计算： GRPO不再训练独立的价值模型。对于每个问题，模型会生成一组（例如G个）响应。每个响应的“优势”不再依赖于预测的价值，而是通过其奖励与该组平均奖励的相对差异来计算（例如，Ajk​=Rjk​−Rˉj​）。这提供了一个更高效、更局部的优势估计。 25
   2. KL散度惩罚： GRPO仍然保留KL散度项。这不仅保证了训练的稳定性，更重要的是，它确保了新策略与初始监督微调（SFT）模型保持对齐，防止模型在优化过程中“漂移”。 23
________________


GRPO的显著优势与实际应用


* 显著优势：
   * 极大地提高了内存效率和计算效率，使得在有限资源下训练大型模型成为可能。 25
   * 简化了RLHF（人类反馈强化学习）流程。
* 实际应用：
   * GRPO已成功应用于DeepSeekMath和DeepSeek-R1等领先的LLM模型。 27
   * 这些模型在数学和通用推理任务中展现出卓越性能，证明了GRPO在提升LLM推理能力方面的强大潜力。 30
[幻灯片8: 总结与问答 - 0.5分钟]


总结与展望


* 从REINFORCE到TRPO，再到PPO，最终到GRPO，我们看到了强化学习算法如何不断演进，以解决新的挑战并适应更复杂的应用场景，特别是大型语言模型的训练。
* GRPO通过其独特的组内相对优势计算和对价值函数的移除，为LLM的强化学习开辟了新的高效路径。
* 未来，强化学习与LLM的融合将持续深入，共同推动AI技术的发展。
感谢大家！现在是问答环节。
________________
引用的著作
1. 什么是强化学习？| NVIDIA 术语表 - 英伟达, 访问时间为 六月 19, 2025， https://www.nvidia.cn/glossary/reinforcement-learning/
2. 异策略深度强化学习中的经验回放研究综述 - 自动化学报, 访问时间为 六月 19, 2025， http://www.aas.net.cn/cn/article/doi/10.16383/j.aas.c220648?viewType=HTML
3. 强化学习基础 - Deepnote, 访问时间为 六月 19, 2025， https://deepnote.com/app/ding/-deb6cadb-4777-4f46-ba81-e57b6a7bf340
4. 什么是强化学习？ - AWS, 访问时间为 六月 19, 2025， https://aws.amazon.com/cn/what-is/reinforcement-learning/
5. 强化学习基础教程, 访问时间为 六月 19, 2025， https://mgubaidullin.github.io/deeplearning4j-docs/cn/reinforcementlearning
6. 强化学习简介— 简单粗暴TensorFlow 2 0.4 beta 文档, 访问时间为 六月 19, 2025， https://tf.wiki/zh_hans/appendix/rl.html
7. 基于值函数和策略梯度的深度强化学习综述 - 计算机学报, 访问时间为 六月 19, 2025， http://cjc.ict.ac.cn/online/onlinepaper/42-6-15-201968180907.pdf
8. 策略梯度算法 - 动手学强化学习, 访问时间为 六月 19, 2025， https://hrl.boyuai.com/chapter/2/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95/
9. Policy Gradient Theorem Explained: A Hands-On Introduction - DataCamp, 访问时间为 六月 19, 2025， https://www.datacamp.com/tutorial/policy-gradient-theorem
10. Mastering Policy Gradient Methods - Number Analytics, 访问时间为 六月 19, 2025， https://www.numberanalytics.com/blog/mastering-policy-gradient-methods
11. 多智能体深度强化学习的若干关键科学问题 - 自动化学报, 访问时间为 六月 19, 2025， http://www.aas.net.cn/cn/article/doi/10.16383/j.aas.c200159?viewType=HTML
12. From REINFORCE to GRPO: Evolution of Policy Optimization in ..., 访问时间为 六月 19, 2025， https://astanahub.com/en/blog/ot-podkrepleniia-k-grpo-evoliutsiia-politiki-optimizatsii-obucheniia-s-podkrepleniem
13. LLM Optimization: Optimizing AI with GRPO, PPO, and DPO - Analytics Vidhya, 访问时间为 六月 19, 2025， https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/
14. 基于样本效率优化的深度强化学习方法综述 - 软件学报, 访问时间为 六月 19, 2025， https://www.jos.org.cn/html/2022/11/6391.htm
15. Trust-Region Policy Optimization - UT Computer Science, 访问时间为 六月 19, 2025， https://www.cs.utexas.edu/~pstone/Courses/394Rspring22/resources/week14-trpo.pdf
16. TRPO 算法 - 动手学强化学习, 访问时间为 六月 19, 2025， https://hrl.boyuai.com/chapter/2/trpo%E7%AE%97%E6%B3%95/
17. Proximal Policy Optimization - Toloka, 访问时间为 六月 19, 2025， https://toloka.ai/blog/proximal-policy-optimization/
18. What is Trust Region Policy Optimization (TRPO)? - Milvus, 访问时间为 六月 19, 2025， https://milvus.io/ai-quick-reference/what-is-trust-region-policy-optimization-trpo
19. Trust Region Meta Learning for Policy Optimization, 访问时间为 六月 19, 2025， https://proceedings.mlr.press/v191/occorso22a/occorso22a.pdf
20. 深度探索：机器学习中的Trust Region Policy Optimization (TRPO)算法原理及其应用, 访问时间为 六月 19, 2025， https://blog.csdn.net/qq_51320133/article/details/137868927
21. Trust Region Policy Optimization - 深度强化学习Spinning Up 项目中文版, 访问时间为 六月 19, 2025， https://spinningup.qiwihui.com/zh_CN/latest/algorithms/trpo.html
22. Simple Policy Optimization | OpenReview, 访问时间为 六月 19, 2025， https://openreview.net/forum?id=MOEqbKoozj
23. Training Large Language Models: From TRPO to GRPO | Towards ..., 访问时间为 六月 19, 2025， https://towardsdatascience.com/training-large-language-models-from-trpo-to-grpo/
24. PPO 算法 - 动手学强化学习, 访问时间为 六月 19, 2025， https://hrl.boyuai.com/chapter/2/ppo%E7%AE%97%E6%B3%95/
25. Why GRPO is Important and How it Works - Oxen.ai, 访问时间为 六月 19, 2025， https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/
26. Deep dive into Group Relative Policy Optimization (GRPO) - Community.aws, 访问时间为 六月 19, 2025， https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo
27. Theory Behind GRPO - AI Engineering Academy, 访问时间为 六月 19, 2025， https://aiengineering.academy/LLM/TheoryBehindFinetuning/GRPO/
28. The math behind DeepSeek: A deep dive into group relative policy optimization - BytePlus, 访问时间为 六月 19, 2025， https://www.byteplus.com/en/topic/376446
29. RL Training For Math Reasoning - Perplexity, 访问时间为 六月 19, 2025， https://www.perplexity.ai/hub/blog/rl-training-for-math-reasoning
30. Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback - Powerdrill, 访问时间为 六月 19, 2025， https://powerdrill.ai/discover/summary-critique-grpo-advancing-llm-reasoning-with-natural-cmbjv716u33yu07rab5i8va7o
31. Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback - arXiv, 访问时间为 六月 19, 2025， https://arxiv.org/html/2506.03106v2
32. Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training - arXiv, 访问时间为 六月 19, 2025， https://arxiv.org/html/2505.22257v1
33. Takeaways from the DeepSeek-R1 model - DEV Community, 访问时间为 六月 19, 2025， https://dev.to/aws/takeaways-from-the-deepseek-r1-model-2dli
34. Bite: How Deepseek R1 was trained - Philschmid, 访问时间为 六月 19, 2025， https://www.philschmid.de/deepseek-r1
35. Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach - arXiv, 访问时间为 六月 19, 2025， https://arxiv.org/pdf/2503.21819?
36. [2502.01652] Hybrid Group Relative Policy Optimization: A Multi-Sample Approach to Enhancing Policy Optimization - arXiv, 访问时间为 六月 19, 2025， https://arxiv.org/abs/2502.01652